# Streaming-LLM with OpenAI Triton
OpenAI triton implementation of streaming LLM

You can now batch inference with streaming LLM and backward :D
